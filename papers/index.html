<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Amirhossein's Website</title>

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@100&family=Ubuntu:wght@300&display=swap" rel="stylesheet">

  <!-- Font-awesome -->
  <script defer src="https://use.fontawesome.com/releases/v5.0.7/js/all.js"></script>

  <!-- bootstrap css -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

  <link rel="stylesheet" href="../css/styles.css">

  <!-- bootstrap javascript -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.2/dist/umd/popper.min.js" integrity="sha384-IQsoLXl5PILFhosVNubq5LC7Qb9DXgDA9i+tQ8Zj3iwWAwPtgFTxbJ8NT4GN1R8p" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.min.js" integrity="sha384-cVKIPhGWiC2Al4u+LWgxfKTRIcfu0JTxR+EQDz/bgldoEyl4H0zUF0QKbrJ0EcQF" crossorigin="anonymous"></script>

</head>

<body>

<section id="title">

    <div class="container-fluid-papers">

      <!-- Nav Bar -->
      <nav class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
        <a class="navbar-brand" href="../index.html">Amirhossein Kardoost</a>

        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo03" aria-controls="navbarTogglerDemo03" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarTogglerDemo03">
          <ul class="navbar-nav ms-auto">
            <li class="navbar-item">
              <a class="nav-link" href="#self-supervised">Papers</a>
            </li>
          </ul>
        </div>
      </nav>

    </div>
  </section>

  <section id="papers-studied">
    <div class="container-fluid">
      <h3 class="subsection-heading-papers" id="self-supervised">Self Supervised</h3>  
      <div class="row">
        <div class="card">
            <div class="card-body">
              <h5 class="card-title">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</h5>
              <h6 class="card-subtitle mb-2 text-muted">Adrien Bardes, Jean Ponce, and Yann LeCun, ICLR 2022</h6>
              <p class="card-text paper-info">The advantage of using self-supervised learning is that is does not require the human made labels for the inputs. 
                  To train such models, the pair of similar images are given to the Siamese network so that their representation are similar. This is the same for
                  human eye, black and white or the color image of one specific object would not change its meaning. The Siamese networks suffer from the mode collapse 
                  where the two branches can learn to produce constant and identical output vectors. With this they satisfy the similarity condition without learning 
                  anything useful. There are different approaches to train such models, either by making asymmetric networks, momentum encoders, or utilizing the 
                  contrastive loss function by pushing away non-similar inputs. 
                  <br>
                  <br>
                  In this paper, the authors suggest that any of the changes are not required and all 
                  depends on the proper definition of the loss function. The loss function contains three terms, variance term, invariance term, and covariance term. 
                  <br>
                  <br>
                  The variance term discourages the mode collapse by maximizing the variance along the batch dimension for every embedding dimension. The invariance term
                  is a similarity metric which needs to be minimized between the similar inputs. The covariance term makes the covariance matrix of the embeddings to be as 
                  close to the diagonal as possible. With this term, the model is encouraged to spread the information over the embedding dimension. The authors provide the 
                  code as well to experiment on the CIFAR-10 dataset. </p>
              <a target="_blank" href="https://arxiv.org/pdf/2105.04906.pdf" class="card-link">Paper</a>
              <a target="_blank" href="https://colab.research.google.com/drive/1Bi-nbAbwFpEUOxfM1qXfOOeigE7Ur2F_?usp=sharing" class="card-link">Colab Code</a>
            </div>
        </div>
      </div>

      <br>
      
    </div>
    
  </section>

  <footer class="white-section" id="footer">
    <div class="container-fluid">
      <p id="contact-info">âœ‰ amirhossein.kardoost AT xfel.eu</p>
      <a class="link-website" href="https://amirhk-dev.github.io/">amirhk-dev.github.io</a>
      <div id="copyright">
        <hr>
        <p class="copyright-text">Powered by Bootstrap.</p>
      </div>
    </div>
  </footer>

</body>

